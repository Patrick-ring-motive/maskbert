<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <title>DistilBERT Masked Fill - Pure TF.js</title>
</head>

<body>
  <h1>DistilBERT Masked Fill (TF.js WebGL)</h1>
  <input id="text" size="60" value="The capital of France is [MASK]." />
  <button id="predict">Predict</button>
  <pre id="output">Loading...</pre>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
  <script>
    (async () => {
      try {
    //    await import("https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core");
     //   await import("https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl");
        class WordPieceTokenizer {
          constructor(vocabulary, unknownToken = '[UNK]', suffixIndicator = '##') {
            this.vocabulary = new Set(vocabulary);
            this.unknownToken = unknownToken;
            this.suffixIndicator = suffixIndicator;
          }

          tokenize(text) {
            const tokens = [];
            // Normalize text and split into words, removing empty strings.
            const words = text.toLowerCase().split(/\s+/).filter(word => word.length > 0);

            for (const word of words) {
              let subwordTokens = [];
              let remainingWord = word;
              let isUnknown = false;

              // Greedily find the longest matching subword
              while (remainingWord.length > 0) {
                let longestMatch = '';
                let matchLength = 0;

                for (let i = remainingWord.length; i > 0; i--) {
                  let candidate = remainingWord.substring(0, i);

                  // The first token of a word should not have the suffix indicator
                  if (subwordTokens.length > 0) {
                    candidate = this.suffixIndicator + candidate;
                  }

                  if (this.vocabulary.has(candidate)) {
                    longestMatch = candidate;
                    matchLength = i;
                    break; // Found the longest match, move on
                  }
                }

                if (longestMatch.length > 0) {
                  subwordTokens.push(longestMatch);
                  remainingWord = remainingWord.substring(matchLength);
                } else {
                  // No match found for the current remaining part of the word
                  isUnknown = true;
                  break;
                }
              }

              if (isUnknown) {
                tokens.push(this.unknownToken);
              } else {
                tokens.push(...subwordTokens);
              }
            }
            return tokens;
          }
        }


        async function fetchText() {
          return await (await fetch(...arguments)).text();
        }

        const getVocab = async () => {
          let text = String(await fetchText('vocab.txt'));
          text = `${text}\n${text.toLowerCase()}`;
          return [...new Set(text.split('\n').map(x => x.trim()).filter(x => x))];
        };

        let model;
        const maskToken = '[MASK]';
        const maskId = 103; // BERT base uncased and DistilBERT use 103 for [MASK]

        // Minimal vocab
        const invVocab = await getVocab();
        const vocab = Object.fromEntries(Object.entries(invVocab).map(([k, v]) => [v, k]));
    

        const tokenizer = new WordPieceTokenizer(invVocab);

        function tokenize(text) {
          return tokenizer.tokenize(text);
        }

        async function loadModel() {
          tf.setBackend('webgl');
          await tf.ready();
          model = await tf.loadGraphModel('./distilbert-tfjs/model.json');
          document.getElementById('output').textContent = 'Model loaded.';
        }

        const loaded = loadModel();

        async function run() {
          try {
            await loaded;
            const text = document.getElementById('text').value;
            const tokens = tokenize(text);
            const inputIdsArray = tokens.map(x => vocab[x]);
            console.log({ inputIdsArray });
            const inputIds = tf.tensor([inputIdsArray], undefined, 'int32');
            const attentionMask = tf.tensor([inputIdsArray.map(() => 1)], undefined, 'int32');

            const result = await model.executeAsync({
              'input_ids': inputIds,
              'attention_mask': attentionMask
            });
            const resultArray = result.arraySync();
            console.log({ resultArray });
            const logits = resultArray[0]; // [seq_len, vocab_size]
            const maskIndex = inputIdsArray.indexOf(maskId);
            let maskLogits = logits[maskIndex];
            if (!maskLogits) {
              console.log(`${maskIndex} not found`);
              maskLogits = logits[0];// fallback if not found
            }

            const top5 = [...maskLogits.map((score, id) => ({ id, score }))]
              .sort((a, b) => b.score - a.score).filter(({ id }) => invVocab[id]).slice(0, 5)
              .map(({ id, score }) => `${invVocab[id] ?? '[UNK]'} (${score.toFixed(3)})`)
              .join('\n');

            document.getElementById('output').textContent = top5;
          } catch (e) {
            document.getElementById('output').textContent += '\n' + e.message;
          }
        }

        document.getElementById('predict').addEventListener('click', run);
      } catch (e) {
        document.getElementById('output').textContent += '\n' + e.message;
        console.error(e);
      }

    })();
  </script>
</body>

</html>